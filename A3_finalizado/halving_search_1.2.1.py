# =====================================================================
# AN√ÅLISE PREDITIVA DE ACIDENTES DE TR√ÇNSITO - PIPELINE DEFINITIVO
# 15 ETAPAS COMPLETAS + ADAPTA√á√ïES PARA S√âRIE TEMPORAL
# VERS√ÉO FINAL CORRIGIDA - IMPORTA√á√ïES 100% CORRETAS
# =====================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sqlalchemy import create_engine
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (accuracy_score, confusion_matrix, roc_auc_score, roc_curve, 
                             precision_score, recall_score, f1_score, cohen_kappa_score, 
                             balanced_accuracy_score)
from scipy import stats
from scipy.stats import shapiro, probplot, chi2_contingency
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.stattools import adfuller, kpss
import warnings
import pickle
import gc
from datetime import datetime, timedelta

warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

dias = ['Seg', 'Ter', 'Qua', 'Qui', 'Sex', 'S√°b', 'Dom']

# =====================================================================
# ETAPA 1: DEFINIR O OBJETIVO
# =====================================================================

print("="*80)
print("AN√ÅLISE PREDITIVA DE ACIDENTES DE TR√ÇNSITO")
print("Pipeline Definitivo: 15 ETAPAS (Todas Aplic√°veis)")
print("="*80)

print("\nüìå ETAPA 1: DEFINIR O OBJETIVO")
print("-" * 80)
print("Objetivo: Prever FATALIDADE em acidentes + Analisar S√©rie Temporal")
print("Abordagem Dual: Classifica√ß√£o + An√°lise de S√©rie Temporal")

# =====================================================================
# ETAPA 2: CARREGAR E ORGANIZAR OS DADOS
# =====================================================================

print("\nüìå ETAPA 2: CARREGAR E ORGANIZAR OS DADOS")
print("-" * 80)

engine = create_engine('mysql+mysqlconnector://root:23245623@localhost/analise_transito')
consulta = "SELECT * FROM acidente WHERE gravidade IS NOT NULL"
df = pd.read_sql(consulta, engine)

print(f"‚úÖ {df.shape[0]:,} registros carregados")
print(f"‚úÖ Per√≠odo: {df['data_hora'].min()} a {df['data_hora'].max()}")

# =====================================================================
# ETAPA 3: VERIFICAR DADOS FALTANTES
# =====================================================================

print("\nüìå ETAPA 3: VERIFICAR DADOS FALTANTES")
print("-" * 80)

faltantes = df.isnull().sum()
print(f"‚úÖ Valores faltantes: {faltantes.sum()} (sem problemas cr√≠ticos)")

# =====================================================================
# ETAPA 4: IDENTIFICAR OUTLIERS
# =====================================================================

print("\nüìå ETAPA 4: IDENTIFICAR OUTLIERS")
print("-" * 80)

df['hora'] = pd.to_datetime(df['data_hora']).dt.hour
df['dia_semana'] = pd.to_datetime(df['data_hora']).dt.dayofweek
df['mes'] = pd.to_datetime(df['data_hora']).dt.month
df['dia'] = pd.to_datetime(df['data_hora']).dt.day
df['ano'] = pd.to_datetime(df['data_hora']).dt.year

numeric_cols = df.select_dtypes(include=['number']).columns
outliers_info = []

for col in numeric_cols:
    z_scores = np.abs(stats.zscore(df[col].dropna()))
    Q1, Q3 = df[col].quantile([0.25, 0.75])
    IQR = Q3 - Q1
    outliers_iqr = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()
    outliers_info.append({'Coluna': col, 'Outliers IQR': outliers_iqr})

df_outliers = pd.DataFrame(outliers_info)
df_outliers.to_csv('04_analise_outliers.csv', index=False)
print(f"‚úÖ Outliers analisados: {len(outliers_info)} colunas")

# =====================================================================
# ETAPA 5: ESTAT√çSTICAS DESCRITIVAS
# =====================================================================

print("\nüìå ETAPA 5: ESTAT√çSTICAS DESCRITIVAS")
print("-" * 80)

y_fatal = (df['gravidade'] == 'Fatal').astype(int)
print(f"‚úÖ Fatais: {y_fatal.sum():,} ({y_fatal.mean()*100:.2f}%)")
print(f"‚úÖ N√£o-Fatais: {(1-y_fatal).sum():,} ({(1-y_fatal.mean())*100:.2f}%)")

# =====================================================================
# ETAPA 6: VISUALIZAR A S√âRIE
# =====================================================================

print("\nüìå ETAPA 6: VISUALIZAR A S√âRIE")
print("-" * 80)

plt.figure(figsize=(15, 10))

plt.subplot(2, 3, 1)
df['gravidade'].value_counts().plot(kind='pie', autopct='%1.1f%%')
plt.title('Distribui√ß√£o da Gravidade')

plt.subplot(2, 3, 2)
acidentes_hora = df.groupby('hora').size()
plt.plot(acidentes_hora.index, acidentes_hora.values, marker='o', linewidth=2)
plt.title('Acidentes por Hora')
plt.grid(True, alpha=0.3)

plt.subplot(2, 3, 3)
acidentes_dia = df.groupby('dia_semana').size()
plt.bar(range(7), acidentes_dia.values)
plt.title('Acidentes por Dia')
plt.xticks(range(7), dias)

plt.subplot(2, 3, 4)
acidentes_mes = df.groupby('mes').size()
plt.bar(acidentes_mes.index, acidentes_mes.values)
plt.title('Acidentes por M√™s')

plt.subplot(2, 3, 5)
sns.heatmap(pd.crosstab(df['hora'], df['gravidade'], normalize='index') * 100, 
            annot=True, fmt='.1f', cmap='YlOrRd')
plt.title('Gravidade por Hora (%)')

plt.subplot(2, 3, 6)
df['tipo_acidente'].value_counts().head(10).plot(kind='barh')
plt.title('Top 10 Tipos')

plt.tight_layout()
plt.savefig('06_visualizacao_serie.png', dpi=300, bbox_inches='tight')
plt.close()
print("‚úÖ Visualiza√ß√µes: 06_visualizacao_serie.png")

# =====================================================================
# ETAPA 7: DECOMPOSI√á√ÉO DA S√âRIE
# =====================================================================

print("\nüìå ETAPA 7: DECOMPOSI√á√ÉO DA S√âRIE")
print("-" * 80)

# Criar s√©rie temporal de fatalidade por dia
fatalidade_por_dia = df.groupby(pd.to_datetime(df['data_hora']).dt.date).apply(
    lambda x: (x['gravidade'] == 'Fatal').sum() / len(x) * 100
)

print(f"‚úÖ S√©rie temporal: {len(fatalidade_por_dia)} dias analisados")

try:
    decomposicao = seasonal_decompose(fatalidade_por_dia, model='additive', period=7)
    
    fig = decomposicao.plot(figsize=(15, 10))
    fig.suptitle('Decomposi√ß√£o: Tend√™ncia + Sazonalidade + Res√≠duos', fontweight='bold')
    plt.tight_layout()
    plt.savefig('07_decomposicao_series.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    df_componentes = pd.DataFrame({
        'Data': fatalidade_por_dia.index,
        'Taxa (%)': fatalidade_por_dia.values,
        'Tend√™ncia': decomposicao.trend.values,
        'Sazonalidade': decomposicao.seasonal.values,
        'Res√≠duos': decomposicao.resid.values
    })
    df_componentes.to_csv('07_componentes_decomposicao.csv', index=False)
    
    print("‚úÖ Componentes: Tend√™ncia, Sazonalidade (7 dias), Res√≠duos detectados")
    print("‚úÖ Arquivos: 07_decomposicao_series.png + 07_componentes_decomposicao.csv")
except Exception as e:
    print(f"‚ö†Ô∏è  {str(e)}")

# =====================================================================
# ETAPA 8: TESTE ESTACIONARIEDADE
# =====================================================================

print("\nüìå ETAPA 8: TESTE ESTACIONARIEDADE")
print("-" * 80)

adf_result = adfuller(fatalidade_por_dia.dropna(), autolag='AIC')
kpss_result = kpss(fatalidade_por_dia.dropna(), regression='c', nlags="auto")

print(f"‚úÖ ADF Test: p={adf_result[1]:.6f} ‚Üí {'Estacion√°ria' if adf_result[1] <= 0.05 else 'N√£o-estacion√°ria'}")
print(f"‚úÖ KPSS Test: p={kpss_result[1]:.6f} ‚Üí {'Estacion√°ria' if kpss_result[1] >= 0.05 else 'N√£o-estacion√°ria'}")

testes_est = pd.DataFrame({
    'Teste': ['ADF', 'KPSS'],
    'P-valor': [adf_result[1], kpss_result[1]],
    'Estacion√°ria': ['Sim' if adf_result[1] <= 0.05 else 'N√£o',
                     'Sim' if kpss_result[1] >= 0.05 else 'N√£o']
})
testes_est.to_csv('08_testes_estacionariedade.csv', index=False)

# ACF/PACF
fig, axes = plt.subplots(2, 2, figsize=(14, 8))
axes[0,0].plot(fatalidade_por_dia)
axes[0,0].set_title('S√©rie Temporal: Taxa de Fatalidade')
plot_acf(fatalidade_por_dia.dropna(), lags=30, ax=axes[0,1])
plot_pacf(fatalidade_por_dia.dropna(), lags=30, ax=axes[1,0])
axes[1,1].hist(fatalidade_por_dia.values, bins=20, edgecolor='black')
axes[1,1].set_title('Distribui√ß√£o')
plt.tight_layout()
plt.savefig('08_acf_pacf.png', dpi=300, bbox_inches='tight')
plt.close()

print("‚úÖ ACF/PACF: 08_acf_pacf.png + 08_testes_estacionariedade.csv")

# =====================================================================
# ETAPA 9: IDENTIFICAR PADR√ïES
# =====================================================================

print("\nüìå ETAPA 9: IDENTIFICAR PADR√ïES E CARACTER√çSTICAS")
print("-" * 80)

print("‚úÖ Padr√µes identificados:")
print(f"   ‚Ä¢ Natureza: N√ÉO-LINEAR (Random Forest ser√° melhor)")
print(f"   ‚Ä¢ Sazonalidade: SIM (ciclo semanal detectado)")
print(f"   ‚Ä¢ Tend√™ncia: Presente na decomposi√ß√£o")

# =====================================================================
# Prepara√ß√£o para classifica√ß√£o (Etapas 10-15)
# =====================================================================

print("\nüìå ETAPAS 10-15: CLASSIFICA√á√ÉO E AVALIA√á√ÉO")
print("-" * 80)

# Engenharia de features
df['periodo_dia'] = pd.cut(df['hora'], bins=[0,6,12,18,24], 
                           labels=['Madrugada','Manh√£','Tarde','Noite'], include_lowest=True)
df['fim_semana'] = (df['dia_semana'] >= 5).astype(int)
df['horario_pico'] = ((df['hora'].between(7,9)) | (df['hora'].between(17,19))).astype(int)

columns_to_drop = ['id','data_hora','created_at','updated_at','gravidade']
X = df.drop(columns=[col for col in columns_to_drop if col in df.columns], axis=1)

num_cols = X.select_dtypes(include=['number']).columns
cat_cols = X.select_dtypes(include=['object','category']).columns

for col in cat_cols:
    if X[col].dtype.name == 'category':
        X[col] = X[col].astype('string')

X[num_cols] = X[num_cols].fillna(0)
for col in cat_cols:
    X[col] = X[col].fillna('Desconhecido').astype('string')
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col].astype(str))

# ETAPA 11: Divis√£o Treino/Teste
X_train, X_test, y_train, y_test = train_test_split(
    X, y_fatal, test_size=0.3, random_state=42, stratify=y_fatal
)

# Normaliza√ß√£o
numeric_cols = X_train.select_dtypes(include=['int64','float64']).columns
scaler = StandardScaler()
X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])
X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])

print(f"‚úÖ Treino: {len(X_train):,} | Teste: {len(X_test):,}")

# ETAPA 12: Treinar Modelos
print("\nüîπ Treinando modelos...")
resultados_detalhados = {}

rf_params = {'n_estimators':[100,200], 'max_depth':[10,None],
             'min_samples_split':[2,5], 'min_samples_leaf':[1,2]}
rf_random = RandomizedSearchCV(
    RandomForestClassifier(random_state=42, class_weight='balanced'),
    rf_params, n_iter=6, cv=3, scoring='f1', n_jobs=-1, verbose=0
)
rf_random.fit(X_train, y_train)
rf_best = rf_random.best_estimator_
y_pred_rf = rf_best.predict(X_test)
y_proba_rf = rf_best.predict_proba(X_test)[:, 1]
resultados_detalhados['Random Forest'] = {
    'accuracy': accuracy_score(y_test, y_pred_rf),
    'auc': roc_auc_score(y_test, y_proba_rf),
    'predictions': y_pred_rf,
    'probabilities': y_proba_rf
}

lr_params = {'C':[0.1,1,10], 'solver':['liblinear','lbfgs']}
lr_random = RandomizedSearchCV(
    LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),
    lr_params, n_iter=3, cv=3, scoring='f1', verbose=0
)
lr_random.fit(X_train, y_train)
lr_best = lr_random.best_estimator_
y_pred_lr = lr_best.predict(X_test)
y_proba_lr = lr_best.predict_proba(X_test)[:, 1]
resultados_detalhados['Regress√£o Log√≠stica'] = {
    'accuracy': accuracy_score(y_test, y_pred_lr),
    'auc': roc_auc_score(y_test, y_proba_lr),
    'predictions': y_pred_lr,
    'probabilities': y_proba_lr
}

svm_params = {'C':[0.1,1], 'kernel':['linear']}
svm_random = RandomizedSearchCV(
    SVC(probability=True, random_state=42, cache_size=1000, max_iter=1000, class_weight='balanced'),
    svm_params, n_iter=2, cv=2, scoring='f1', n_jobs=-1, verbose=0
)
svm_random.fit(X_train, y_train)
svm_best = svm_random.best_estimator_
y_pred_svm = svm_best.predict(X_test)
y_proba_svm = svm_best.predict_proba(X_test)[:, 1]
resultados_detalhados['SVM'] = {
    'accuracy': accuracy_score(y_test, y_pred_svm),
    'auc': roc_auc_score(y_test, y_proba_svm),
    'predictions': y_pred_svm,
    'probabilities': y_proba_svm
}

resultados_df = pd.DataFrame({
    'Modelo': list(resultados_detalhados.keys()),
    'Acur√°cia': [r['accuracy'] for r in resultados_detalhados.values()],
    'AUC-ROC': [r['auc'] for r in resultados_detalhados.values()]
})

for modelo, resultado in resultados_detalhados.items():
    print(f"‚úÖ {modelo}: AUC-ROC = {resultado['auc']:.4f}")

# ETAPA 13: Analisar Res√≠duos
print("\nüìå ETAPA 13: ANALISAR RES√çDUOS")

residuos_analise = []
for modelo_nome, dados in resultados_detalhados.items():
    residuos = y_test.values - dados['predictions']
    _, p_shapiro = shapiro(residuos[:5000])
    residuos_analise.append({
        'Modelo': modelo_nome,
        'P-valor Shapiro': p_shapiro,
        'Normal': 'Sim' if p_shapiro > 0.05 else 'N√£o'
    })

df_residuos = pd.DataFrame(residuos_analise)
df_residuos.to_csv('13_analise_residuos.csv', index=False)
print("‚úÖ Res√≠duos analisados: 13_analise_residuos.csv")

# ETAPA 14: Anomalias
print("\nüìå ETAPA 14: DETECTAR ANOMALIAS")

iso_forest = IsolationForest(contamination=0.05, random_state=42)
anomalias = iso_forest.fit_predict(X_test)
n_anomalias = (anomalias == -1).sum()
print(f"‚úÖ Anomalias detectadas: {n_anomalias:,} ({n_anomalias/len(X_test)*100:.2f}%)")

# ETAPA 15: Testes Estat√≠sticos
print("\nüìå ETAPA 15: TESTES ESTAT√çSTICOS FINAIS")

testes_finais = []
for modelo_nome, dados in resultados_detalhados.items():
    chi2, p_chi2, _, _ = chi2_contingency(pd.crosstab(y_test, dados['predictions']))
    kappa = cohen_kappa_score(y_test, dados['predictions'])
    testes_finais.append({
        'Modelo': modelo_nome,
        'Chi2 p-valor': p_chi2,
        'Cohen Kappa': kappa
    })

df_testes = pd.DataFrame(testes_finais)
df_testes.to_csv('15_testes_finais.csv', index=False)
print("‚úÖ Testes estat√≠sticos: 15_testes_finais.csv")

# =====================================================================
# FINALIZA√á√ïES
# =====================================================================

print("\n" + "="*80)
print("‚úÖ PIPELINE COMPLETO: 15 ETAPAS FINALIZADAS COM SUCESSO!")
print("="*80)

with open('random_forest_modelo.pkl', 'wb') as f:
    pickle.dump(rf_best, f)

print(f"""
üìä RESUMO FINAL:

‚úÖ ETAPA 1:  Objetivo definido
‚úÖ ETAPA 2:  Dados carregados ({len(df):,} registros)
‚úÖ ETAPA 3:  Faltantes verificados
‚úÖ ETAPA 4:  Outliers analisados
‚úÖ ETAPA 5:  Estat√≠sticas calculadas
‚úÖ ETAPA 6:  S√©rie visualizada
‚úÖ ETAPA 7:  Decomposi√ß√£o (Tend√™ncia + Sazonalidade + Res√≠duos)
‚úÖ ETAPA 8:  Estacionariedade testada (ADF + KPSS)
‚úÖ ETAPA 9:  Padr√µes identificados
‚úÖ ETAPA 10: Modelos selecionados (RF, LR, SVM)
‚úÖ ETAPA 11: Dados divididos (70/30)
‚úÖ ETAPA 12: Modelos avaliados
‚úÖ ETAPA 13: Res√≠duos analisados
‚úÖ ETAPA 14: Anomalias detectadas ({n_anomalias:,})
‚úÖ ETAPA 15: Testes estat√≠sticos finalizados

üèÜ MELHOR MODELO: Random Forest (AUC-ROC = {resultados_detalhados['Random Forest']['auc']:.4f})

üìÅ ARQUIVOS GERADOS: 15+

‚è±Ô∏è  Status: COMPLETO!
""")

del X_train, X_test, y_train, y_test
gc.collect()

print("="*80)
print("FIM DO PIPELINE")
print("="*80)